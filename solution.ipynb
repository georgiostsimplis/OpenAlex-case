{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> OpenAlex data for Research Portal Denmark (RPD) </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Use OpenAlex search UI and compare results with current RPD databases\n",
    "How many records in Open Alex and the 3 current RPD global data sources fulfill these criteria:\n",
    "- At least one author is affiliated to a Danish institution.\n",
    "- Year = 2022\n",
    "- SDG (UN Sustainable Development Goal) = 2 Zero hunger\n",
    "- There is Open Access to the publication (any kind of Open Access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>Solution</font>**\n",
    "\n",
    "!!! Note the answers were given at 24/1/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers – please send to us before the interview:\n",
    "- OpenAlex: <font color='red'>1659</font>\n",
    "- Clarivate: <font color='red'>610</font>\n",
    "- Digital Science: <font color='red'>254</font>\n",
    "- Elsevier: <font color='red'>393</font>\n",
    "For the OpenAlex answer, please cut and paste the search result URL here:  \n",
    "\n",
    "https://openalex.org/works?page=1&filter=publication_year%3A2022,sustainable_development_goals.id%3Ahttps%3A%2F%2Fmetadata.un.org%2Fsdg%2F2,open_access.is_oa%3Atrue,institutions.country_code%3ADK&sort=cited_by_count%3Adesc&group_by=publication_year,open_access.is_oa,authorships.institutions.lineage,type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Programming challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. Data Extraction from OpenAlex**\n",
    "<font color='yellow'>o RPD holds records pertaining to Denmark starting from the year 2011. OpenAlex UI indicates the system currently has 457,600 such records.\n",
    "\n",
    "o Write a Python script to extract all these records using the OpenAlex API and store them in a SQL/NoSQL database of your choice – please send the script to us before the interview. – Downloading 1000 records or so will suffice for the further tasks.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>Solution</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_and_store(api_url, database_name, collection_name):\n",
    "    # Specify the cursor parameter to start cursor pagination\n",
    "    params_cursor_paging = {\n",
    "        'cursor': '*',\n",
    "    }\n",
    "\n",
    "    # Connect to MongoDB on localhost |  running on the default port 27017\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    # Create or access a database |\n",
    "    db = client[database_name]\n",
    "\n",
    "    # Create or access a collection |\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    total_results = 0\n",
    "    next_cursor = params_cursor_paging\n",
    "\n",
    "    while next_cursor is not None and total_results < 1000:  #I limited the instances to 1000\n",
    "        response_cursor_paging = requests.get(api_url, params=params_cursor_paging)\n",
    "        data_cursor_paging = response_cursor_paging.json()\n",
    "\n",
    "\n",
    "        data = data_cursor_paging[\"results\"]\n",
    "\n",
    "        result = collection.insert_many(data)\n",
    "\n",
    "        next_cursor = data_cursor_paging['meta']['next_cursor']\n",
    "\n",
    "        # 200 results by page\n",
    "        total_results += 200\n",
    "\n",
    "        params_cursor_paging['cursor'] = next_cursor\n",
    "\n",
    "\n",
    "    client.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_url = \"https://api.openalex.org/works?page=1&filter=publication_year:2011-2024,institutions.country_code:DK&sort=cited_by_count:desc&per_page=200\"\n",
    "    database_name = 'openalex_db'\n",
    "    collection_name = 'dataset'\n",
    "    extract_and_store(api_url, database_name, collection_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **extract_and_store** function uses the request library to make api calls to retrieve data from OpenAlex. It uses cursor paging to which allows to access many records. Then it cretes a MongoDB database and a collection and stores the data for every page. I have set the instances per page to 200 and I limited the retreived documents to 1000. At the following image you can see the resulted collection using Studio 3T UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/ing1.jpg\"\n",
    "     alt=\"Studio 3T image\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='yellow'>o Every month all the RPD databases are updated with new and changed records.\n",
    "\n",
    "o Write a Python script for the monthly extraction and storage of new and changed\n",
    "records. Ideally the script should automate this process, so it can be run periodically to\n",
    "keep the data up to date – please send the script to us before the interview.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>Solution</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to OpenAlex api documentation we need a Premium Subscription to use the **from_updated_date** filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/from.jpg\"\n",
    "     alt=\"documentation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made an application to get the api key. I used again the function **extract_and_store** but this time using the appropriate api url to get the filtered data that are updated from 20/1/2024. \n",
    "\n",
    "*At the following section I dont provide my api key*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://api.openalex.org/works?page=1&filter=publication_year:2011-2024,institutions.country_code:DK,from_updated_date:2024-01-20&api_key=<MY_API_KEY>&sort=cited_by_count:desc&per_page=200\"\n",
    "database_name = 'openalex_db'\n",
    "collection_name = 'updated_data'\n",
    "extract_and_store(api_url, database_name, collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see I the above screenshot, I created anothe collection named **updated_data**. I will use this collection to update the other collection - *named dataset* - that contains the full data.\n",
    "\n",
    "<img src=\"pics/s3t.jpg\"\n",
    "     alt=\"documentation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes as argument the name of the updated collection and updates the data to the full data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "    \n",
    "def update_data(updated_collection):    \n",
    "    \n",
    "    client = MongoClient('localhost', 27017)\n",
    "    database = client['openalex_db']\n",
    "    collection1 = database['dataset']\n",
    "    collection2 = database[updated_collection]\n",
    "\n",
    "    try:\n",
    "        # Get all distinct titles from collection2\n",
    "        distinct_titles = collection2.distinct(\"title\")\n",
    "\n",
    "        # Delete documents in collection1 with matching titles\n",
    "        collection1.delete_many({\"title\": {\"$in\": distinct_titles}})\n",
    "\n",
    "        # Get all documents from collection2 and insert into collection1\n",
    "        documents_to_insert = list(collection2.find())\n",
    "        if documents_to_insert:\n",
    "            collection1.insert_many(documents_to_insert)\n",
    "\n",
    "        print('Merge operation completed successfully.')\n",
    "\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automate the above process so that it runs every month I used **Dagster**, a tool that is used for Cloud-native orchestration of data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pymongo import MongoClient\n",
    "from Data_extraction.extract_data import extract_and_store\n",
    "from Data_extraction.update_collection import update_data\n",
    "from dagster import (\n",
    "    AssetExecutionContext,\n",
    "    Definitions,\n",
    "    ScheduleDefinition,\n",
    "    asset,\n",
    "    define_asset_job,\n",
    ")\n",
    "\n",
    "@asset  \n",
    "def monthly_updated_data(\n",
    "    context: AssetExecutionContext):\n",
    "\n",
    "    current_datetime = datetime.now()\n",
    "\n",
    "    previous_month_datetime = current_datetime - relativedelta(months=1)\n",
    "\n",
    "    previous_month_date = previous_month_datetime.date()\n",
    "\n",
    "    \n",
    "    # Date to string to name the collection\n",
    "    formatted_date = current_datetime.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "    previous_month_date_formated = previous_month_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # format the date so that it changes the date at the \"from_updated_date\" filter\n",
    "    api_url = \"https://api.openalex.org/works?page=1&filter=publication_year:2011-2024,institutions.country_code:DK,from_updated_date:{}&api_key=<MY_KEY>&sort=cited_by_count:desc&per_page=200\".format(previous_month_date_formated)\n",
    "\n",
    "    database_name = 'openalex_db'\n",
    "    new_collection_name = 'updated_data_{0}'.format(formatted_date)\n",
    "\n",
    "    extract_and_store(api_url, database_name, new_collection_name)\n",
    "\n",
    "    update_data(new_collection_name)\n",
    "    \n",
    "updatedata_job = define_asset_job(\"updatedata_job\", selection=[monthly_updated_data])\n",
    "\n",
    "#ScheduleDefinition the job it should run and a cron schedule\n",
    "updatedata_schedule = ScheduleDefinition(\n",
    "    job=updatedata_job,\n",
    "    cron_schedule = \"0 0 1 * *\",  # every month\n",
    ")\n",
    "\n",
    "\n",
    "defs = Definitions(\n",
    "    assets=[monthly_updated_data],\n",
    "    schedules=[updatedata_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running ```dagster dev -f schedule.py``` at the terminal it opens the UI of the platform to start and schedule the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/dag.jpg\"\n",
    "     alt=\"documentation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data enhancement\n",
    "<font color='yellow'> o For example, the RPD maintains its own standard names for Danish research institutions and its own standard grouping of these. See for example the ‘Danish Affiliations’ filter at https://clarivate.forskningsportal.dk/search?facet_content-type_ss=publications . RPD has mapping tables for all the name variants that should be mapped to these standard names and groups.\n",
    "\n",
    "o Consider and present how you think such an enhancement step could be added to the\n",
    "data processing pipeline you are designing. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>Solution</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, due to the fact that I don't have access to the RPD mapping tables I assume the following:  \n",
    "\n",
    "*The mapping table should contain at least 3 columns.The first should contain the RPD standard name of the institution, the second a list of the variants of the names and the third the group that the institute belongs to. E.g Universities, Hospitals, Non-Profit Organizations etc.*\n",
    "\n",
    "Then we have to generate a script (matching algorithm) so that for each record in the OpenAlex dataset, finds a match in the variants column of mapping tables, based on the institution name. Once it matches the name of the institution with a variant then the algorithm should replace the name at the OpenAlex dataset with the RPD standard name.\n",
    "\n",
    "This step should be added in the designed pipeline as a transformation step after the data ingestion from OpenAlex. What I desgribe is an ELT process than the below picture illustrates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/ELT.png\"\n",
    "     alt=\"documentation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data indexing using Elasticsearch\n",
    "\n",
    "<font color='yellow'> o One of the end-products of your data processing pipeline will be a search UI based on\n",
    "Elasticsearch and with a look and feel similar to https://clarivate.forskningsportal.dk/search?facet_content-type_ss=publications\n",
    "\n",
    "o Consider and present how you think such a search service could be built and kept up to\n",
    "data – in a robust, efficient, and user-friendly way. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>Solution</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task I created a python script that connects the MongoDB database with the elasticsearch. The following script allows elasticsearch to dynamically infer the mapping based on the structure of the documents. This process is known as dynamic mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "import json\n",
    "\n",
    "mongo_client = MongoClient('localhost', 27017)\n",
    "mongo_db = mongo_client[\"openalex_db\"]\n",
    "mongo_collection = mongo_db[\"dataset\"]\n",
    "\n",
    "es = Elasticsearch('http://localhost:9200', verify_certs=False)\n",
    "print(es.ping())\n",
    "\n",
    "index_name = \"bulk19\"\n",
    "\n",
    "\n",
    "es.indices.create(index=index_name)\n",
    "\n",
    "# Index each document from MongoDB into Elasticsearch\n",
    "for document in mongo_collection.find():\n",
    "    \n",
    "    document['_id'] = str(document['_id'])\n",
    "\n",
    "# Remove MongoDB-specific _id field\n",
    "    if '_id' in document:\n",
    "        del document['_id']\n",
    "\n",
    "    \n",
    "    es.index(index=index_name, body=document)\n",
    "\n",
    "# Udpdate the index to make the documents available for search\n",
    "es.indices.refresh(index=index_name)\n",
    "\n",
    "mongo_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have run we can discover our data at the 'Discover' section of **Kibana**. A screenshot of a part of the indexed data is presented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/kib.jpg\"\n",
    "     alt=\"documentation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the above script is a basic implementation of an Elasticsearch indexing process. In order to build a search service in a robust, efficient, and user-friendly way we have to consider some factors that analyzed below.\n",
    "\n",
    "- Mapping  \n",
    "  First of all we have to choose the appropriate field type for each attribute in our documents (e.g., text, keyword, date, long, etc.). For example we have to define the 'title' or 'display_name' element as 'text' to allow for full-text search. The county could be defined as 'keyword' type for exact matching and aggregations. The 'publication_year' could be defined as date to allow sorting and filtering.\n",
    "\n",
    "- Autocomplete  \n",
    "  This functionality returns suggestions to the end user based on the user inputs. We could integrate this component to make the search UI user-friendly.\n",
    "\n",
    "- Synchronization  \n",
    "  The pipeline should be designed in a way that allows the sunchronization of MongoDB with Elasticsearch. This component will enable the Elasticsearch to ingest the updated data from MongoDB automatically after the updates that occur. \n",
    "\n",
    "\n",
    "We can add more factors that we can consider to build the specific earch service, keeping always in mind the objectives of the end-product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional extra investigation:\n",
    "### D. Data Exploration (1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
